
These distances are calculated for 20 possible combinations of atoms, specifically: ('C', 'C'), ('H', 'O'), ('O', 'O'), ('S', 'S'), ('C', 'Cl'), ('C', 'S'), ('H', 'H'), ('C', 'N'), ('H', 'C'), ('N', 'Cl'), ('C', 'O'), ('Cl', 'Cl'), ('N', 'S'), ('N', 'N'), ('Cl', 'S'), ('O', 'Cl'), ('H', 'Cl'), ('H', 'S'), ('O', 'S'), ('H', 'N'), and ('O', 'N'). These atom combinations serve as features for our modeling efforts, encapsulating crucial information about the molecular structure and properties. However, we encountered the challenge of dealing with a considerable number of sparse features, especially when considering all three types of molecular distances (min, max, and mean). If we were to include all of these distances as features, it would lead to a high-dimensional dataset with 60 sparse features (20 combinations x 3 distances). This could adversely impact the performance and interpretability of our model, rendering it less efficient in capturing meaningful patterns within the data.
To address this concern and improve the dimensionality of our dataset, we conducted three separate trials. In Trial 1, we utilized the mean molecular distance as the sole feature for our model, leading to 20 sparse features. Similarly, in Trial 2, we selected the maximum molecular distance as the feature, also yielding 20 sparse features. Lastly, in Trial 3, we considered the minimum molecular distance as the feature, resulting in another set of 20 sparse features. By conducting these trials, we aim to perform a comparative study to identify the most suitable feature representation for our modeling efforts. This comparative approach will enable us to determine which type of molecular distance provides the most informative and discriminative characteristics for the model's predictive performance.
Following the feature extraction and trial analysis described earlier, we further refined the dataset to improve its suitability for modeling. One crucial step in this process was the parsing of the composition column, which initially existed as a dictionary representation. By parsing this column, we obtained the count of molecules for each of the six additional features, namely 'H', 'N', 'C', 'S', 'CL', and 'O'. These counts served as valuable additional features, contributing vital information about the molecular composition and distribution of atoms within the molecules.
To ensure that our model focused on the most relevant information, we made the decision to drop nonessential features such as 'types_of_atoms', 'method', and 'basis_set'. By removing these features, which were deemed nonworthy for predictive purposes, we streamlined the dataset and mitigated the risk of introducing noise or unnecessary complexity into the modeling process. In preparation for training our model, we identified the target variable as "AE" (Atomization Energy). All remaining fields, after the exclusion of nonworthy features, were considered as potential predictor variables or features for the model. These features encompassed diverse molecular properties, including various atomic compositions, distances, and other descriptors that offered insights into molecular behavior. To assess the model's performance accurately, we adopted the standard practice of train-test split, allocating 80% of the data for training and the remaining 20% for testing. 
In pursuit of an effective modeling approach, we employed Decision Trees as the foundational technique for our analysis. To accommodate different levels of model fidelity and comprehend the tradeoff between accuracy and computational cost, we constructed models at nine distinct fidelity levels. Each fidelity level corresponded to a specific depth assigned to the Decision Tree model. Deeper trees tended to capture intricate patterns in the data, potentially yielding higher accuracy but with increased computational overhead. During the modeling process, we computed various evaluation metrics to assess the performance of the Decision Tree models. Metrics such as Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE) were utilized to quantify the model's predictive accuracy and its deviation from the actual values. These metrics allowed us to gauge the effectiveness of each model at different fidelity levels and identify the optimal depth that struck the right balance between accuracy and computational efficiency. Alongside the evaluation metrics, we also logged the computation time for each model at different fidelity levels. This enabled us to gain insights into the computational cost associated with varying depths of the Decision Tree models. By analyzing the tradeoff between accuracy and computation time, we aimed to identify the fidelity level that offered the best compromise, delivering reasonably accurate predictions while minimizing computational resources.
In pursuit of further enhancement, we adopted ensembling techniques as part of our multi-fidelity modeling strategy. Specifically, we employed a Random Forest ensemble, a powerful technique that combines decision trees from all nine fidelity levels. The Random Forest model aggregated the predictions made by individual Decision Trees, leveraging the collective intelligence of diverse models with varying depths.

